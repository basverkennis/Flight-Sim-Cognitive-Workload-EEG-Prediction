{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipympl\n",
    "%matplotlib widget \n",
    "# makes sure plots are saved in folder and not displayed in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def cm_plot(cm_value, model, total=False):\n",
    "    classes = ['Low mental workload', 'High mental workload']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm_value, interpolation='nearest', cmap='coolwarm')\n",
    "    plt.title(f'Confusion Matrix - {model}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # Add the values to the confusion matrix plot\n",
    "    thresh = cm_value.max() / 2.0\n",
    "    for i in range(cm_value.shape[0]):\n",
    "        for j in range(cm_value.shape[1]):\n",
    "            plt.text(j, i, format(cm_value[i, j], 'd'), ha='center', va='center',\n",
    "                     color='white' if cm_value[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    output_file = f'/Flight-Sim-Cognitive-Workload-EEG-Prediction/results/confusion_matrices/cmplot_{model}.png'\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fronto-Parietal Phase Locking Value (PLV) Connectivity function\n",
    "### Retrieving the average and standardized version of the Fronto-Parietal Phase Locking Value (PLV) Connectivity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fronto_parietal_averagePLV(data):\n",
    "    '''\n",
    "    Computes the mean connectivity between frontal and parietal channels bidirectionally.\n",
    "    \n",
    "    Parameters:\n",
    "        data: Connectivity data for each channel.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: (average) fronto-parietal connectivity values for each channel and for each frequency band (alpha, beta, theta).\n",
    "    '''\n",
    "    \n",
    "    # Alpha mean PLV values\n",
    "    F7_A = np.mean(data[4:9])\n",
    "    F3_A = np.mean(data[12:17])\n",
    "    Fz_A = np.mean(data[19:24])\n",
    "    F4_A = np.mean(data[25:30])\n",
    "    F8_A = np.mean(data[30:35])\n",
    "    P3_A = np.mean(np.concatenate((data[4:5], data[12:13], data[19:20], data[25:26], data[30:31])))\n",
    "    Pz_A = np.mean(np.concatenate((data[5:6], data[13:14], data[20:21], data[26:27], data[31:32])))\n",
    "    P4_A = np.mean(np.concatenate((data[6:7], data[14:15], data[21:22], data[27:28], data[32:33])))\n",
    "    PO7_A = np.mean(np.concatenate((data[7:8], data[15:16], data[22:23], data[28:29], data[33:34])))\n",
    "    PO8_A = np.mean(np.concatenate((data[8:9], data[16:17], data[23:24], data[29:30], data[34:35])))\n",
    "    \n",
    "    # Beta mean PLV values\n",
    "    F7_B = np.mean(data[4+45:9+45])\n",
    "    F3_B = np.mean(data[12+45:17+45])\n",
    "    Fz_B = np.mean(data[19+45:24+45])\n",
    "    F4_B = np.mean(data[25+45:30+45])\n",
    "    F8_B = np.mean(data[30+45:35+45])\n",
    "    P3_B = np.mean(np.concatenate((data[4+45:5+45], data[12+45:13+45], data[19+45:20+45], data[25+45:26+45], data[30+45:31+45])))\n",
    "    Pz_B = np.mean(np.concatenate((data[5+45:6+45], data[13+45:14+45], data[20+45:21+45], data[26+45:27+45], data[31+45:32+45])))\n",
    "    P4_B = np.mean(np.concatenate((data[6+45:7+45], data[14+45:15+45], data[21+45:22+45], data[27+45:28+45], data[32+45:33+45])))\n",
    "    PO7_B = np.mean(np.concatenate((data[7+45:8+45], data[15+45:16+45], data[22+45:23+45], data[28+45:29+45], data[33+45:34+45])))\n",
    "    PO8_B = np.mean(np.concatenate((data[8+45:9+45], data[16+45:17+45], data[23+45:24+45], data[29+45:30+45], data[34+45:35+45])))\n",
    "    \n",
    "    # Theta mean PLV values\n",
    "    F7_T = np.mean(data[4+90:9+90])\n",
    "    F3_T = np.mean(data[12+90:17+90])\n",
    "    Fz_T = np.mean(data[19+90:24+90])\n",
    "    F4_T = np.mean(data[25+90:30+90])\n",
    "    F8_T = np.mean(data[30+90:35+90])\n",
    "    P3_T = np.mean(np.concatenate((data[4+90:5+90], data[12+90:13+90], data[19+90:20+90], data[25+90:26+90], data[30+90:31+90])))\n",
    "    Pz_T = np.mean(np.concatenate((data[5+90:6+90], data[13+90:14+90], data[20+90:21+90], data[26+90:27+90], data[31+90:32+90])))\n",
    "    P4_T = np.mean(np.concatenate((data[6+90:7+90], data[14+90:15+90], data[21+90:22+90], data[27+90:28+90], data[32+90:33+90])))\n",
    "    PO7_T = np.mean(np.concatenate((data[7+90:8+90], data[15+90:16+90], data[22+90:23+90], data[28+90:29+90], data[33+90:34+90])))\n",
    "    PO8_T = np.mean(np.concatenate((data[8+90:9+90], data[16+90:17+90], data[23+90:24+90], data[29+90:30+90], data[34+90:35+90])))\n",
    "    \n",
    "    min_value = np.min([F7_A, F3_A, Fz_A, F4_A, F8_A, P3_A, Pz_A, P4_A, PO7_A, PO8_A, F7_B, F3_B, Fz_B, F4_B, F8_B, P3_B, Pz_B, P4_B, PO7_B, PO8_B, F7_T, F3_T, Fz_T, F4_T, F8_T, P3_T, Pz_T, P4_T, PO7_T, PO8_T])\n",
    "    max_value = np.max([F7_A, F3_A, Fz_A, F4_A, F8_A, P3_A, Pz_A, P4_A, PO7_A, PO8_A, F7_B, F3_B, Fz_B, F4_B, F8_B, P3_B, Pz_B, P4_B, PO7_B, PO8_B, F7_T, F3_T, Fz_T, F4_T, F8_T, P3_T, Pz_T, P4_T, PO7_T, PO8_T])\n",
    "    \n",
    "    PLVs_alpha = [(x - min_value) / (max_value - min_value) * 2 - 1 for x in [F7_A, F3_A, Fz_A, F4_A, F8_A, P3_A, Pz_A, P4_A, PO7_A, PO8_A]]    \n",
    "    PLVs_theta = [(x - min_value) / (max_value - min_value) * 2 - 1 for x in [F7_B, F3_B, Fz_B, F4_B, F8_B, P3_B, Pz_B, P4_B, PO7_B, PO8_B]]\n",
    "    PLVs_beta = [(x - min_value) / (max_value - min_value) * 2 - 1 for x in [F7_T, F3_T, Fz_T, F4_T, F8_T, P3_T, Pz_T, P4_T, PO7_T, PO8_T]]\n",
    "    \n",
    "    data_x = np.concatenate((PLVs_alpha, PLVs_beta, PLVs_theta))\n",
    "\n",
    "    return data_x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Ensemble Model\n",
    "\n",
    "### 42 spectral features\n",
    "### Recursive Feature Elimination (RFE), GridSearch, Stacked Classifier (SVM, Logistic Regression & Random Forest, with meta-model SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data \n",
    "\n",
    "directory = '/Flight-Sim-Cognitive-Workload-EEG-Prediction/data'\n",
    "concatenated_X, concatenated_Y = [], []\n",
    "for subdir in sorted(os.listdir(directory))[1:]:\n",
    "    for file in sorted(os.listdir(os.path.join(directory, subdir))):\n",
    "        # print(directory.split('/')[-1], subdir, file)\n",
    "        data = np.load(os.path.join(directory, subdir, file))\n",
    "        X = data['X'][135:] # extract spectral features\n",
    "        Y = data['Y']\n",
    "        concatenated_X.append(X)\n",
    "        concatenated_Y.append(Y)\n",
    "\n",
    "X = concatenated_X\n",
    "Y = np.concatenate(concatenated_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination - RFE\n",
    "\n",
    "num_features = 8\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "feature_selection_counts = np.zeros(np.array(X).shape[1])\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(skf.split(X,Y), start=1):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    estimator = svm.LinearSVC(max_iter=30000, random_state=42)\n",
    "    selector = RFE(estimator, n_features_to_select=num_features)\n",
    "    selector.fit(X_train, Y_train)\n",
    "    \n",
    "    feature_selection_counts += selector.support_.astype(int)\n",
    "\n",
    "feature_selection_frequency = feature_selection_counts / num_folds\n",
    "best_feature_indices = np.argsort(-feature_selection_frequency)[:num_features]\n",
    "X_selected = np.array(X)[:, best_feature_indices]\n",
    "\n",
    "print(\"Feature selection frequency across folds:\", feature_selection_frequency)\n",
    "print(\"Best feature indices:\", best_feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning - GridSearchCV\n",
    "\n",
    "scoring = make_scorer(accuracy_score)\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    # RandomForest parameters\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__min_samples_leaf': [1, 2],\n",
    "    'rf__bootstrap': [True],\n",
    "    \n",
    "    # LogisticRegression parameters\n",
    "    'lr__C': [0.01, 0.1, 1.0],\n",
    "    'lr__solver': ['liblinear'],\n",
    "    \n",
    "    # SVC (base) parameters\n",
    "    'svm_base__C': [0.01, 1.0, 10],\n",
    "    'svm_base__kernel': ['linear'],\n",
    "    'svm_base__gamma': ['scale', 'auto'],\n",
    "    \n",
    "    # Final estimator (meta-model) parameters\n",
    "    'final_estimator__C': [0.01, 1.0, 10],\n",
    "    'final_estimator__kernel': ['linear'],\n",
    "    'final_estimator__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_selection_counts = {param: {value: 0 for value in values} for param, values in param_grid.items()}\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(skf.split(X_selected, Y), start=1):\n",
    "    X_train, X_test = np.array(X_selected)[train_index], np.array(X_selected)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('lr', LogisticRegression(max_iter=30000, random_state=42)),\n",
    "    ('svm_base', svm.SVC(probability=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    meta_model = svm.SVC(random_state=42)\n",
    "\n",
    "    stacked_classifier = StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=meta_model\n",
    "    )\n",
    "\n",
    "    stackedmodel_grid_search = GridSearchCV(stacked_classifier, param_grid=param_grid, scoring=scoring, cv=8, verbose=1, n_jobs=-1)\n",
    "    stackedmodel_grid_search.fit(X_train, Y_train)\n",
    "    best_stackedmodel_params = stackedmodel_grid_search.best_params_\n",
    "    \n",
    "    for param, best_value in best_stackedmodel_params.items():\n",
    "        param_selection_counts[param][best_value] += 1\n",
    "    \n",
    "average_selected_params = {}\n",
    "for param, value_counts in param_selection_counts.items():\n",
    "    most_common_value = max(value_counts, key=value_counts.get)\n",
    "    average_selected_params[param] = most_common_value\n",
    "\n",
    "print(\"Most frequently selected hyperparameters across folds:\")\n",
    "for param, value in average_selected_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "baseline_model_param = average_selected_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Classifier\n",
    "\n",
    "scoring = make_scorer(accuracy_score)\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "f1_scores, accuracy_scores, precision_scores, recall_scores, cm_scores = [],[],[],[],[]\n",
    "\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=average_selected_params['rf__n_estimators'],\n",
    "        max_depth=average_selected_params['rf__max_depth'],\n",
    "        min_samples_split=average_selected_params['rf__min_samples_split'],\n",
    "        min_samples_leaf=average_selected_params['rf__min_samples_leaf'],\n",
    "        bootstrap=average_selected_params['rf__bootstrap'], \n",
    "        random_state=42\n",
    "    )),\n",
    "    ('lr', LogisticRegression(\n",
    "        C=average_selected_params['lr__C'],\n",
    "        solver=average_selected_params['lr__solver'],\n",
    "        max_iter=30000, \n",
    "        random_state=42\n",
    "    )),\n",
    "    ('svm_base', svm.SVC(\n",
    "        C=average_selected_params['svm_base__C'],\n",
    "        kernel=average_selected_params['svm_base__kernel'],\n",
    "        gamma=average_selected_params['svm_base__gamma'],\n",
    "        probability=True, \n",
    "        random_state=42\n",
    "    ))\n",
    "    ]\n",
    "\n",
    "meta_model = svm.SVC(\n",
    "    C=average_selected_params['final_estimator__C'],\n",
    "    kernel=average_selected_params['final_estimator__kernel'],\n",
    "    gamma=average_selected_params['final_estimator__gamma'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "stacked_classifier = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=meta_model\n",
    ")\n",
    "    \n",
    "for k, (train_index, test_index) in enumerate(skf.split(X_selected, Y), start=1):\n",
    "    X_train, X_test = np.array(X_selected)[train_index], np.array(X_selected)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    stacked_classifier.fit(X_train, Y_train)\n",
    "    Y_pred = stacked_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred, zero_division=1))\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision_score(Y_test, Y_pred, zero_division=1))\n",
    "    recall_scores.append(recall_score(Y_test, Y_pred, zero_division=1))\n",
    "    cm_scores.append(confusion_matrix(Y_test, Y_pred))\n",
    "\n",
    "    print(f'Fold {k} - Accuracy: {round(accuracy, 2)}')\n",
    "\n",
    "print(' ')\n",
    "print('Accuracy:', '\\n', f'Mean: {round(np.mean(accuracy_scores), 2)}.', f'Std: {round(np.std(accuracy_scores), 2)}.', f'Max: {round(np.max(accuracy_scores), 2)}.', f'Min: {round(np.min(accuracy_scores), 2)}.')\n",
    "print(' ')\n",
    "print('F1:', '\\n', f'Mean: {round(np.mean(f1_scores), 2)}.', f'Std: {round(np.std(f1_scores), 2)}.', f'Max: {round(np.max(f1_scores), 2)}.', f'Min: {round(np.min(f1_scores), 2)}.')\n",
    "print(' ')\n",
    "print('Precision:', '\\n', f'Mean: {round(np.mean(precision_scores), 2)}.', f'Std: {round(np.std(precision_scores), 2)}.', f'Max: {round(np.max(precision_scores), 2)}.', f'Min: {round(np.min(precision_scores), 2)}.')\n",
    "print(' ')\n",
    "print('Recall:', '\\n', f'Mean: {round(np.mean(recall_scores), 2)}.', f'Std: {round(np.std(recall_scores), 2)}.', f'Max: {round(np.max(recall_scores), 2)}.', f'Min: {round(np.min(recall_scores), 2)}.')\n",
    "print(' ')\n",
    "\n",
    "combined_cm = np.sum(cm_scores, axis=0)\n",
    "cm_plot(combined_cm, model='Baseline')\n",
    "\n",
    "baseline_model_acc = accuracy_scores\n",
    "baseline_mean = round(np.mean(accuracy_scores), 2)\n",
    "baseline_std = round(np.std(accuracy_scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print features selected by the model\n",
    "\n",
    "ranked_features = pd.DataFrame(columns=['Frequency_Band', 'Feature_type'])\n",
    "node_names_orig = ['F7', 'F3', 'Fz', 'F4', 'F8', 'T7', 'Cz', 'T8', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "combinations_total = node_names_orig*3\n",
    "alpha_spec, beta_spec, theta_spec = [], [], []\n",
    "\n",
    "for i in best_feature_indices:\n",
    "    new_row = None\n",
    "    if i < 14:\n",
    "        alpha_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band': ['Alpha'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 14 and i < 28:\n",
    "        beta_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band': ['Beta'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 28 and i < 42:\n",
    "        theta_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band': ['Theta'], 'Feature_type': [combinations_total[i]]})\n",
    "\n",
    "    if new_row is not None:\n",
    "        ranked_features = pd.concat([ranked_features, new_row], ignore_index=True)\n",
    "\n",
    "print(f'Features ranked order: {ranked_features}')\n",
    "print(' ')\n",
    "print(f'Number of chosen spectral features per frequency band: \\nTotal: {len(alpha_spec)+len(beta_spec)+len(theta_spec)}')\n",
    "print(' ')\n",
    "print(f'Spectral features per frequency band: \\nAlpha: {alpha_spec} \\nBeta: {beta_spec} \\nTheta: {theta_spec}')\n",
    "\n",
    "BaselineModel_SpectralPower_Chosen = [alpha_spec, beta_spec, theta_spec]\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Ensemble Model\n",
    "### 42 spectral features + 30 frontal-parietal PLV (phase-locking value) features\n",
    "### Recursive Feature Elimination (RFE), GridSearch, Stacked Classifier (SVM, Logistic Regression & Random Forest, with meta-model SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data \n",
    "\n",
    "directory = '/Flight-Sim-Cognitive-Workload-EEG-Prediction/data'\n",
    "concatenated_X, concatenated_Y = [], []\n",
    "for subdir in sorted(os.listdir(directory))[1:]:\n",
    "    for file in sorted(os.listdir(os.path.join(directory, subdir))):\n",
    "        # print(directory.split('/')[-1], subdir, file)\n",
    "        data = np.load(os.path.join(directory, subdir, file))\n",
    "        X_PLV = fronto_parietal_averagePLV(data['X'][:135]) # Output: 30 bidirectional PLV features (10 Channels x 3 frequency bands)\n",
    "        X_Freq = data['X'][135:] # spectral power features\n",
    "        X = np.concatenate((X_PLV, X_Freq))\n",
    "        Y = data['Y']\n",
    "        concatenated_X.append(X)\n",
    "        concatenated_Y.append(Y)\n",
    "\n",
    "X = concatenated_X\n",
    "Y = np.concatenate(concatenated_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination - RFE\n",
    "\n",
    "num_features = 8\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "feature_selection_counts = np.zeros(np.array(X).shape[1])\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(skf.split(X,Y), start=1):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    estimator = svm.LinearSVC(max_iter=30000, random_state=42)\n",
    "    selector = RFE(estimator, n_features_to_select=num_features)\n",
    "    selector.fit(X_train, Y_train)\n",
    "    \n",
    "    feature_selection_counts += selector.support_.astype(int)\n",
    "\n",
    "feature_selection_frequency = feature_selection_counts / num_folds\n",
    "best_feature_indices = np.argsort(-feature_selection_frequency)[:num_features]\n",
    "X_selected = np.array(X)[:, best_feature_indices]\n",
    "\n",
    "print(\"Feature selection frequency across folds:\", feature_selection_frequency)\n",
    "print(\"Best feature indices:\", best_feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning - GridSearchCV\n",
    "\n",
    "scoring = make_scorer(accuracy_score)\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    # RandomForest parameters\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__min_samples_leaf': [1, 2],\n",
    "    'rf__bootstrap': [True],\n",
    "    \n",
    "    # LogisticRegression parameters\n",
    "    'lr__C': [0.01, 0.1, 1.0],\n",
    "    'lr__solver': ['liblinear'],\n",
    "    \n",
    "    # SVC (base) parameters\n",
    "    'svm_base__C': [0.01, 1.0, 10],\n",
    "    'svm_base__kernel': ['linear'],\n",
    "    'svm_base__gamma': ['scale', 'auto'],\n",
    "    \n",
    "    # Final estimator (meta-model) parameters\n",
    "    'final_estimator__C': [0.01, 1.0, 10],\n",
    "    'final_estimator__kernel': ['linear'],\n",
    "    'final_estimator__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_selection_counts = {param: {value: 0 for value in values} for param, values in param_grid.items()}\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(skf.split(X_selected, Y), start=1):\n",
    "    X_train, X_test = np.array(X_selected)[train_index], np.array(X_selected)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('lr', LogisticRegression(max_iter=30000, random_state=42)),\n",
    "    ('svm_base', svm.SVC(probability=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    meta_model = svm.SVC(random_state=42)\n",
    "\n",
    "    stacked_classifier = StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=meta_model\n",
    "    )\n",
    "\n",
    "    stackedmodel_grid_search = GridSearchCV(stacked_classifier, param_grid=param_grid, scoring=scoring, cv=8, verbose=1, n_jobs=-1)\n",
    "    stackedmodel_grid_search.fit(X_train, Y_train)\n",
    "    best_stackedmodel_params = stackedmodel_grid_search.best_params_\n",
    "    \n",
    "    for param, best_value in best_stackedmodel_params.items():\n",
    "        param_selection_counts[param][best_value] += 1\n",
    "    \n",
    "average_selected_params = {}\n",
    "for param, value_counts in param_selection_counts.items():\n",
    "    most_common_value = max(value_counts, key=value_counts.get)\n",
    "    average_selected_params[param] = most_common_value\n",
    "\n",
    "print(\"Most frequently selected hyperparameters across folds:\")\n",
    "for param, value in average_selected_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "connectivity_model_param = average_selected_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Classifier\n",
    "\n",
    "scoring = make_scorer(accuracy_score)\n",
    "num_folds = 8\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "f1_scores, accuracy_scores, precision_scores, recall_scores, cm_scores = [],[],[],[],[]\n",
    "\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=average_selected_params['rf__n_estimators'],\n",
    "        max_depth=average_selected_params['rf__max_depth'],\n",
    "        min_samples_split=average_selected_params['rf__min_samples_split'],\n",
    "        min_samples_leaf=average_selected_params['rf__min_samples_leaf'],\n",
    "        bootstrap=average_selected_params['rf__bootstrap'],\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('lr', LogisticRegression(\n",
    "        C=average_selected_params['lr__C'],\n",
    "        solver=average_selected_params['lr__solver'],\n",
    "        max_iter=30000,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('svm_base', svm.SVC(\n",
    "        C=average_selected_params['svm_base__C'],\n",
    "        kernel=average_selected_params['svm_base__kernel'],\n",
    "        gamma=average_selected_params['svm_base__gamma'],\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "    ]\n",
    "\n",
    "meta_model = svm.SVC(\n",
    "    C=average_selected_params['final_estimator__C'],\n",
    "    kernel=average_selected_params['final_estimator__kernel'],\n",
    "    gamma=average_selected_params['final_estimator__gamma'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "stacked_classifier = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=meta_model\n",
    ")\n",
    "    \n",
    "for k, (train_index, test_index) in enumerate(skf.split(X_selected, Y), start=1):\n",
    "    X_train, X_test = np.array(X_selected)[train_index], np.array(X_selected)[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    stacked_classifier.fit(X_train, Y_train)\n",
    "    Y_pred = stacked_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred, zero_division=1))\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision_score(Y_test, Y_pred, zero_division=1))\n",
    "    recall_scores.append(recall_score(Y_test, Y_pred, zero_division=1))\n",
    "    cm_scores.append(confusion_matrix(Y_test, Y_pred))\n",
    "\n",
    "    print(f'Fold {k} - Accuracy: {round(accuracy, 2)}')\n",
    "\n",
    "print(' ')\n",
    "print('Accuracy:', '\\n', f'Mean: {round(np.mean(accuracy_scores), 2)}.', f'Std: {round(np.std(accuracy_scores), 2)}.', f'Max: {round(np.max(accuracy_scores), 2)}.', f'Min: {round(np.min(accuracy_scores), 2)}.')\n",
    "print(' ')\n",
    "print('F1:', '\\n', f'Mean: {round(np.mean(f1_scores), 2)}.', f'Std: {round(np.std(f1_scores), 2)}.', f'Max: {round(np.max(f1_scores), 2)}.', f'Min: {round(np.min(f1_scores), 2)}.')\n",
    "print(' ')\n",
    "print('Precision:', '\\n', f'Mean: {round(np.mean(precision_scores), 2)}.', f'Std: {round(np.std(precision_scores), 2)}.', f'Max: {round(np.max(precision_scores), 2)}.', f'Min: {round(np.min(precision_scores), 2)}.')\n",
    "print(' ')\n",
    "print('Recall:', '\\n', f'Mean: {round(np.mean(recall_scores), 2)}.', f'Std: {round(np.std(recall_scores), 2)}.', f'Max: {round(np.max(recall_scores), 2)}.', f'Min: {round(np.min(recall_scores), 2)}.')\n",
    "print(' ')\n",
    "\n",
    "combined_cm = np.sum(cm_scores, axis=0)\n",
    "cm_plot(combined_cm, model='Connectivity_Model')\n",
    "\n",
    "connectivity_model_acc = accuracy_scores\n",
    "connectivity_mean = round(np.mean(accuracy_scores), 2)\n",
    "connectivity_std = round(np.std(accuracy_scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print features selected by the model\n",
    "\n",
    "ranked_features = pd.DataFrame(columns=['Frequency_Band_&_Feature_set', 'Feature_type'])\n",
    "plv_names = ['F7', 'F3', 'Fz', 'F4', 'F8', 'P3', 'Pz', 'P4', 'PO7', 'PO8']\n",
    "node_names_orig = ['F7', 'F3', 'Fz', 'F4', 'F8', 'T7', 'Cz', 'T8', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "combinations_total = plv_names*3 + node_names_orig*3\n",
    "\n",
    "alpha_plv, beta_plv, theta_plv = [], [], []\n",
    "alpha_spec, beta_spec, theta_spec = [], [], []\n",
    "\n",
    "for i in best_feature_indices:\n",
    "    new_row = None\n",
    "    if i < 10:\n",
    "        alpha_plv.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Alpha-PLV'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 10 and i < 20:\n",
    "        beta_plv.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Beta-PLV'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 20 and i < 30:\n",
    "        theta_plv.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Theta-PLV'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 30 and i < 44:\n",
    "        alpha_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Alpha-Spectral'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 44 and i < 58:\n",
    "        beta_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Beta-Spectral'], 'Feature_type': [combinations_total[i]]})\n",
    "    elif i >= 58 and i < 72:\n",
    "        theta_spec.append(combinations_total[i])\n",
    "        new_row = pd.DataFrame({'Frequency_Band_&_Feature_set': ['Theta-Spectral'], 'Feature_type': [combinations_total[i]]})\n",
    "\n",
    "    if new_row is not None:\n",
    "        ranked_features = pd.concat([ranked_features, new_row], ignore_index=True)\n",
    "\n",
    "print(f'Ranked order: \\n {ranked_features}')        \n",
    "print(' ')\n",
    "print('Number of Connectivity features chosen: ', len(alpha_plv) + len(beta_plv) + len(theta_plv))\n",
    "print(f'PLV features per frequency band: \\nAlpha: {alpha_plv} \\nBeta: {beta_plv} \\nTheta: {theta_plv}')\n",
    "print(' ')\n",
    "print('Number of Spectral features chosen: ', len(alpha_spec) + len(beta_spec) + len(theta_spec))\n",
    "print(f'Spectral features per frequency band: \\nAlpha: {alpha_spec} \\nBeta: {beta_spec} \\nTheta: {theta_spec}')\n",
    "\n",
    "Connectivity_Chosen = [alpha_plv, beta_plv, theta_plv]\n",
    "ConnectivityModel_SpectralPower_Chosen = [alpha_spec, beta_spec, theta_spec]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box plot and Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot accuracy-scores per model.\n",
    "\n",
    "data = [baseline_model_acc, connectivity_model_acc]\n",
    "sns.set(style=\"white\")\n",
    "palette = [\"lightblue\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=data, palette=palette, linewidth=2, boxprops={\"edgecolor\": \"darkblue\"}, medianprops={\"color\": \"darkblue\"}, whiskerprops={\"color\": \"darkblue\"}, capprops={\"color\": \"darkblue\"})\n",
    "plt.xticks(ticks=range(len(['Baseline Model', 'Connectivity Model'])),labels=['Baseline Model', 'Connectivity Model'])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0,1])\n",
    "\n",
    "# Add median scores on the y-axis\n",
    "medians = [np.median(baseline_model_acc), np.median(connectivity_model_acc)]\n",
    "for i, median in enumerate(medians):\n",
    "    plt.text(i, median, f'{median:.2f}', horizontalalignment='center', verticalalignment='bottom', fontdict={'weight': 'bold', 'size': 12}, color='darkblue')\n",
    "\n",
    "plt.title(f'Mean&Std Baseline {baseline_mean, baseline_std}, Mean&Std Connectivity-model {connectivity_mean, connectivity_std}')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "output_file = 'Flight-Sim-Cognitive-Workload-EEG-Prediction/results/boxplot_accuracy.png'\n",
    "plt.savefig(output_file, bbox_inches='tight')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected spectral features and PLV features per model.\n",
    "\n",
    "SpectralPower_Chosen = BaselineModel_SpectralPower_Chosen, ConnectivityModel_SpectralPower_Chosen\n",
    "models = ['Baseline Model', 'Connectivity Model']\n",
    "for name, selected in zip(models, SpectralPower_Chosen):\n",
    "    print(f'Selected spectral features of {name}: {selected}')\n",
    "\n",
    "name = 'Connectivity Model'\n",
    "print(f'Selected Connectivity (PLV) features of {name}: {Connectivity_Chosen}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model with highest accuracy among folds\n",
    "\n",
    "acc_scores = baseline_model_acc, connectivity_model_acc\n",
    "chosen_param = baseline_model_param, connectivity_model_param\n",
    "\n",
    "new_list = []\n",
    "for i in acc_scores:\n",
    "    idx = np.argmax(i)\n",
    "    new_list.append(i[idx])\n",
    "idx1 = np.argmax(new_list)\n",
    "\n",
    "if idx1 == 0:\n",
    "    model = 'Baseline Model' \n",
    "elif idx1 == 1:\n",
    "    model = 'Connectivity Model'\n",
    "print(f'Best performing model based on highest accuracy among folds: {model} \\n max accuracy score: {new_list[idx1]}. \\n mean accuracy score: {np.mean(acc_scores[idx1])}. \\n best performing set of hyperparameters: {chosen_param[idx1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best performing model based on highest mean accuracy over folds.\n",
    "\n",
    "mean_acc_scores = np.mean(acc_scores, axis=1)\n",
    "idx = np.argmax(mean_acc_scores)\n",
    "idx1 = np.argmax(acc_scores[idx])\n",
    "if idx == 0:\n",
    "    model = 'Baseline Model'\n",
    "elif idx == 1:\n",
    "    model = 'Connectivity Model'\n",
    "print(f'Best performing model based on highest mean accuracy: {model} \\n max accuracy score: {acc_scores[idx][idx1]}. \\n mean accuracy score: {np.mean(mean_acc_scores[idx])}. \\n best performing set of hyperparameters: {chosen_param[idx]}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
